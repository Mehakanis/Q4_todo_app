# Example Values File for Todo App Helm Chart
#
# This file demonstrates how to customize the Todo application deployment.
# Copy this file to values-local.yaml and modify the values as needed.
#
# IMPORTANT: values-local.yaml is gitignored and should contain your actual secrets.
# Never commit actual credentials to version control.
#
# Usage:
#   helm upgrade --install todo-app ./k8s/todo-app \
#     --namespace todo \
#     --create-namespace \
#     -f ./k8s/todo-app/values-local.yaml

# nameOverride partially overrides the chart name
# Example: Set to "my-todo" to change resource names
nameOverride: ""

# fullnameOverride fully overrides the generated name
# Example: Set to "my-custom-app" for complete control over resource naming
fullnameOverride: ""

# Frontend configuration for the Next.js application
frontend:
  # replicaCount is the number of frontend pod replicas
  # Increase for higher availability and load distribution
  # Recommended: 2-3 for production, 1 for development
  replicaCount: 2

  # image is the Docker image for the frontend
  # If using external registry: "myregistry.io/todo-frontend:v1.0.0"
  image: "todo-frontend:latest"

  # imagePullPolicy determines when to pull the image
  # Options: Always, IfNotPresent, Never
  # Use "Always" for :latest tags, "IfNotPresent" for specific versions
  imagePullPolicy: "IfNotPresent"

  # resources define CPU and memory allocation
  resources:
    # requests are the guaranteed resources
    # These values determine pod scheduling
    requests:
      memory: "256Mi"  # Increase to "512Mi" for memory-intensive operations
      cpu: "100m"      # Increase to "200m" for better performance

    # limits are the maximum resources allowed
    # Pod will be throttled/killed if it exceeds these limits
    limits:
      memory: "512Mi"  # Increase to "1Gi" if needed
      cpu: "500m"      # Increase to "1000m" for high traffic

  # livenessProbe checks if the container is alive
  # Kubernetes restarts the container if this probe fails
  livenessProbe:
    httpGet:
      path: "/api/health"
      port: 3000
    initialDelaySeconds: 30  # Wait 30s before first check
    periodSeconds: 10        # Check every 10s
    timeoutSeconds: 5        # Probe timeout
    failureThreshold: 3      # Restart after 3 failures

  # readinessProbe checks if the container is ready to serve traffic
  # Traffic is not routed to pod until this probe succeeds
  readinessProbe:
    httpGet:
      path: "/api/ready"
      port: 3000
    initialDelaySeconds: 10  # Wait 10s before first check
    periodSeconds: 5         # Check every 5s
    timeoutSeconds: 3        # Probe timeout
    failureThreshold: 2      # Mark unready after 2 failures

# Backend configuration for the FastAPI application
backend:
  # replicaCount is the number of backend pod replicas
  # Recommended: 2-3 for production, 1 for development
  replicaCount: 2

  # image is the Docker image for the backend
  # If using external registry: "myregistry.io/todo-backend:v1.0.0"
  image: "todo-backend:latest"

  # imagePullPolicy determines when to pull the image
  imagePullPolicy: "IfNotPresent"

  # resources define CPU and memory allocation
  resources:
    # requests are the guaranteed resources
    requests:
      memory: "512Mi"  # Backend needs more memory for AI operations
      cpu: "250m"      # Increase to "500m" for better performance

    # limits are the maximum resources allowed
    limits:
      memory: "1Gi"    # Increase to "2Gi" for large AI models
      cpu: "1000m"     # Increase to "2000m" for high concurrency

  # livenessProbe checks if the container is alive
  livenessProbe:
    httpGet:
      path: "/health"
      port: 8000
    initialDelaySeconds: 45  # Backend needs more startup time
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3

  # readinessProbe checks if the container is ready to serve traffic
  readinessProbe:
    httpGet:
      path: "/ready"
      port: 8000
    initialDelaySeconds: 20  # Wait for database connection
    periodSeconds: 5
    timeoutSeconds: 3
    failureThreshold: 2

# Service configuration for Kubernetes services
service:
  # Frontend service configuration
  frontend:
    # type is the service type
    # NodePort: Exposes service on each node's IP at a static port (30000-32767)
    # LoadBalancer: Exposes service externally using cloud provider's load balancer
    # ClusterIP: Exposes service on cluster-internal IP (default)
    type: "NodePort"

    # port is the service port
    port: 3000

    # targetPort is the container port
    targetPort: 3000

    # nodePort is the port exposed on the node (30000-32767)
    # Change to use a different port if 30300 is already in use
    nodePort: 30300

  # Backend service configuration
  backend:
    # type is ClusterIP for internal access only
    # Backend is not exposed externally - only frontend can access it
    type: "ClusterIP"

    # port is the service port
    port: 8000

    # targetPort is the container port
    targetPort: 8000

# config contains non-sensitive configuration values
# These are injected into pods via ConfigMap
config:
  # backendUrl is the internal URL to reach the backend service
  # Format: http://<service-name>:<port>
  backendUrl: "http://todo-backend:8000"

  # frontendUrl is the external URL to reach the frontend
  # For Minikube: "http://<minikube-ip>:30300"
  # For production: "https://yourdomain.com"
  frontendUrl: "http://localhost:30300"

  # logLevel is the logging level
  # Options: debug, info, warning, error
  # Use "debug" for development, "info" for production
  logLevel: "info"

  # nodeEnv is the Node.js environment
  # Options: development, production, test
  nodeEnv: "production"

# secrets contains sensitive configuration values
# IMPORTANT: Override these values during installation
# NEVER commit actual secrets to version control
secrets:
  # databaseUrl is the PostgreSQL connection string
  # Format: postgresql://username:password@host:port/database
  # Example: "postgresql://admin:secret@postgres.example.com:5432/tododb"
  databaseUrl: "postgresql://user:password@host:5432/dbname"

  # betterAuthSecret is the shared secret for JWT token verification
  # Generate with: openssl rand -base64 32
  # Must be the same value used by your Better Auth server
  betterAuthSecret: "your-secret-key-here-replace-me"

  # openaiApiKey is the OpenAI API key for AI chatbot
  # Get your key from: https://platform.openai.com/api-keys
  # Only required if llmProvider is "openai"
  openaiApiKey: "sk-proj-..."

  # llmProvider is the LLM provider to use
  # Options: openai, openrouter, gemini, groq
  # Each provider requires its corresponding API key below
  llmProvider: "openai"

  # geminiApiKey is the Google Gemini API key
  # Get your key from: https://makersuite.google.com/app/apikey
  # Only required if llmProvider is "gemini"
  geminiApiKey: ""

  # groqApiKey is the Groq API key
  # Get your key from: https://console.groq.com/keys
  # Only required if llmProvider is "groq"
  groqApiKey: ""

  # openrouterApiKey is the OpenRouter API key
  # Get your key from: https://openrouter.ai/keys
  # Only required if llmProvider is "openrouter"
  openrouterApiKey: ""

# Example production configuration:
#
# frontend:
#   replicaCount: 3
#   resources:
#     requests:
#       memory: "512Mi"
#       cpu: "200m"
#     limits:
#       memory: "1Gi"
#       cpu: "1000m"
#
# backend:
#   replicaCount: 3
#   resources:
#     requests:
#       memory: "1Gi"
#       cpu: "500m"
#     limits:
#       memory: "2Gi"
#       cpu: "2000m"
#
# config:
#   logLevel: "info"
#   frontendUrl: "https://todo.example.com"
#
# secrets:
#   databaseUrl: "postgresql://produser:securepass@prod-db.example.com:5432/todoprod"
#   betterAuthSecret: "<32-character-random-string>"
#   openaiApiKey: "sk-proj-..."
#   llmProvider: "openai"
